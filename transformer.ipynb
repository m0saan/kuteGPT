{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "torch.set_printoptions(precision=3, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(d_model=512, d_ff=2048, num_attention_heads=8, num_encoder_layers=6, num_decoder_layers=6, dropout=0.1, eps=1e-06, src_vocab_size=32000, src_seq_len=512, tgt_vocab_size=32000, tgt_seq_len=512)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 512  # Dimensionality of the model\n",
    "    d_ff: int = 2048    # Dimensionality of the feedforward network\n",
    "    num_attention_heads: int = 8  # Number of attention heads\n",
    "    num_encoder_layers: int = 6   # Number of encoder layers\n",
    "    num_decoder_layers: int = 6   # Number of decoder layers\n",
    "    dropout: float = 0.1  # Dropout rate\n",
    "    eps: float = 1e-6  # Epsilon value for layer normalization\n",
    "    \n",
    "    # For source sequence\n",
    "    src_vocab_size: int = 32000  # Source vocabulary size\n",
    "    src_seq_len: int = 512       # Maximum length of source sequence\n",
    "\n",
    "    # For target sequence\n",
    "    tgt_vocab_size: int = 32000  # Target vocabulary size\n",
    "    tgt_seq_len: int = 512       # Maximum length of target sequence\n",
    "\n",
    "# Example usage:\n",
    "config = Config()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(d_model=512, d_ff=2048, num_attention_heads=8, num_encoder_layers=6, num_decoder_layers=6, dropout=0.1, eps=1e-06, src_vocab_size=50000, src_seq_len=400, tgt_vocab_size=45000, tgt_seq_len=500)\n"
     ]
    }
   ],
   "source": [
    "config = Config(src_vocab_size=50000, src_seq_len=400, tgt_vocab_size=45000, tgt_seq_len=500)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = torch.tensor(config.d_model, dtype=torch.float32)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.embeddings = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embeddings(x) * self.d_model.sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 6\n",
    "seq_len = 10\n",
    "even_i = torch.arange(0, d_model, 2)\n",
    "even_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.000, 0.333, 0.667])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_i / d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.000,  21.544, 464.159])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_denominator = torch.pow(10000, even_i/d_model)\n",
    "even_denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_i = torch.arange(1, d_model, 2)\n",
    "odd_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.000, 0.333, 0.667])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(odd_i - 1) / d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.000,  21.544, 464.159])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_denominator = torch.pow(10000, (odd_i-1)/d_model)\n",
    "odd_denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.],\n",
       "        [5.],\n",
       "        [6.],\n",
       "        [7.],\n",
       "        [8.],\n",
       "        [9.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = torch.arange(seq_len, dtype=torch.float32).unsqueeze(-1)\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "evenPE = torch.sin(pos / even_denominator)\n",
    "oddPE = torch.cos(pos / odd_denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.000,  0.000,  0.000],\n",
       "        [ 0.841,  0.046,  0.002],\n",
       "        [ 0.909,  0.093,  0.004],\n",
       "        [ 0.141,  0.139,  0.006],\n",
       "        [-0.757,  0.185,  0.009],\n",
       "        [-0.959,  0.230,  0.011],\n",
       "        [-0.279,  0.275,  0.013],\n",
       "        [ 0.657,  0.319,  0.015],\n",
       "        [ 0.989,  0.363,  0.017],\n",
       "        [ 0.412,  0.406,  0.019]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evenPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.000,  1.000,  1.000],\n",
       "        [ 0.540,  0.999,  1.000],\n",
       "        [-0.416,  0.996,  1.000],\n",
       "        [-0.990,  0.990,  1.000],\n",
       "        [-0.654,  0.983,  1.000],\n",
       "        [ 0.284,  0.973,  1.000],\n",
       "        [ 0.960,  0.961,  1.000],\n",
       "        [ 0.754,  0.948,  1.000],\n",
       "        [-0.146,  0.932,  1.000],\n",
       "        [-0.911,  0.914,  1.000]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oddPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stackedPE = torch.stack([evenPE, oddPE], dim=-1)\n",
    "pe = stackedPE.reshape(seq_len, d_model) # (1, seq_len, d_model)\n",
    "pe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.000,  1.000,  0.000,  1.000,  0.000,  1.000],\n",
       "        [ 0.841,  0.540,  0.046,  0.999,  0.002,  1.000],\n",
       "        [ 0.909, -0.416,  0.093,  0.996,  0.004,  1.000],\n",
       "        [ 0.141, -0.990,  0.139,  0.990,  0.006,  1.000],\n",
       "        [-0.757, -0.654,  0.185,  0.983,  0.009,  1.000],\n",
       "        [-0.959,  0.284,  0.230,  0.973,  0.011,  1.000],\n",
       "        [-0.279,  0.960,  0.275,  0.961,  0.013,  1.000],\n",
       "        [ 0.657,  0.754,  0.319,  0.948,  0.015,  1.000],\n",
       "        [ 0.989, -0.146,  0.363,  0.932,  0.017,  1.000],\n",
       "        [ 0.412, -0.911,  0.406,  0.914,  0.019,  1.000]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.seq_len = config.seq_len\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        position = torch.arange(0, self.seq_len, dtype=torch.float32).unsqueeze(-1) # (seq_len, 1)\n",
    "        dinominator = torch.pow(10000, torch.arange(0, self.d_model, 2, dtype=torch.float32) / self.d_model) # (d_model/2)\n",
    "        oddPE = torch.cos(position / dinominator) # (seq_len, d_model/2)\n",
    "        evenPE = torch.sin(position / dinominator) # (seq_len, d_model/2)\n",
    "        stackedPE = torch.stack([evenPE, oddPE], dim=-1)\n",
    "        pe = stackedPE.reshape(self.seq_len, self.d_model).unsqueeze(0) # (1, seq_len, d_model)\n",
    "        pe.requires_grad = False\n",
    "        \n",
    "        # Register buffer\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config) -> None:\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(config.d_model, config.d_ff)\n",
    "        self.linear2 = nn.Linear(config.d_ff, config.d_model)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config) -> None:\n",
    "        super().__init__()\n",
    "        assert config.d_model % config.num_attention_heads == 0,\\\n",
    "            \"d_model must be divisible by num_attention_heads\"\n",
    "        \n",
    "        self.d_model = config.d_model\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.d_k = self.d_model // self.num_attention_heads\n",
    "        self.d_v = self.d_k\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        self.W_q = nn.Linear(self.d_model, self.d_k)\n",
    "        self.W_k = nn.Linear(self.d_model, self.d_k)\n",
    "        self.W_v = nn.Linear(self.d_model, self.d_v)\n",
    "        self.W_o = nn.Linear(self.num_attention_heads*self.d_v, self.d_model)\n",
    "    \n",
    "    def _scaled_dot_product_attention(query: torch.Tensor, key: torch.Tensor,\n",
    "                                      value: torch.Tensor, mask=None) -> torch.Tensor:\n",
    "        attn_scores = torch.bmm(query, key.transpose(-2, -1)) / self.d_k.sqrt()\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        weights = F.softmax(attn_scores, dim=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        return torch.bmm(weights, value), attn_scores\n",
    "        \n",
    "    def forward(self, x_q: torch.Tensor, x_k: torch.Tensor, x_v: torch.Tensor, \n",
    "                mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        # hidden_state: (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_k)\n",
    "        attention_scores = self._scaled_dot_product_attention(\n",
    "            self.W_q(x_q), self.W_k(x_k), self.W_v(x_v), mask\n",
    "        )\n",
    "        print(f'-------------> attention_scores: {attention_scores.shape}')\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        assert config.d_model % config.num_attention_heads == 0,\\\n",
    "            \"d_model must be divisible by num_attention_heads\"\n",
    "        \n",
    "        self.heads = nn.ModuleList([AttentionHead(config) \n",
    "                                    for _ in range(config.num_attention_heads)])\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.W_o = nn.Linear(self.num_attention_heads*self.d_v, self.d_model)\n",
    "        \n",
    "    def forward(self, x_q: torch.Tensor, x_k: torch.Tensor, x_v: torch.Tensor\n",
    "                , mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        attention_scores = torch.cat([head(x_q, x_k, x_v) for head in self.heads], dim=-1)\n",
    "        return self.W_o(self.dropout(attention_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Risidual(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.layer_norm = nn.LayerNorm(config.d_model, eps=config.eps)\n",
    "        \n",
    "    def forward(selg, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.layer_norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.multi_head_attention_block = MultiHeadAttentionBlock(config)\n",
    "        self.feed_forward_block = FeedForwardBlock(config)\n",
    "        self.risidual1 = Risidual(config)\n",
    "        self.risidual2 = Risidual(config)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.risidual1(x, lambda x: self.multi_head_attention_block(x, x, x, src_mask))\n",
    "        x = self.risidual2(x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder_blocks = nn.ModuleList([EncoderBlock(config)\n",
    "                                             for _ in range(config.num_encoder_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(config.d_model, eps=config.eps)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor: \n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            x = encoder_block(x, mask)\n",
    "        return self.layer_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.masked_multi_head_attention_block = MultiHeadAttentionBlock(config)\n",
    "        self.multi_head_attention_block = MultiHeadAttentionBlock(config)\n",
    "        self.feed_forward_block = FeedForwardBlock(config)\n",
    "        self.risidual1 = Risidual(config)\n",
    "        self.risidual2 = Risidual(config)\n",
    "        self.risidual3 = Risidual(config)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor,\n",
    "                src_mask: torch.Tensor, tgt_mask: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.risidual1(x, lambda x: self.masked_multi_head_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.risidual2(x, lambda x: self.multi_head_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.risidual2(x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.decoder_blocks = nn.ModuleList([DecoderBlock(config)\n",
    "                                             for _ in range(config.num_decoder_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(config.d_model, eps=config.eps)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor,\n",
    "                src_mask: torch.Tensor, tgt_mask: torch.Tensor) -> torch.Tensor:\n",
    "        for decoder_block in self.decoder_blocks:\n",
    "            x = decoder_block(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.layer_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(config.d_model, config.vocab_size)\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.linear(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "\n",
    "    def __init__(self,\n",
    "                 encoder: Encoder,\n",
    "                 decoder: Decoder,\n",
    "                 src_input_embed: InputEmbeddings,\n",
    "                 tgt_input_embed: InputEmbeddings,\n",
    "                 src_pos_embed: PositionalEncoding,\n",
    "                 tgt_pos_embed: PositionalEncoding,\n",
    "                 generator: Generator\n",
    "                ) -> None:\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.generator = generator\n",
    "        self.src_input_embed = src_input_embed\n",
    "        self.tgt_input_embed = tgt_input_embed\n",
    "        self.src_pos_embed = src_pos_embed\n",
    "        self.tgt_pos_embed = src_pos_embed\n",
    "\n",
    "    def encode(self, src: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n",
    "        src_input_embeddings = self.src_input_embed(src)\n",
    "        src_pos_embeddings = self.src_pos_embed(src)\n",
    "        encoder_input = src_input_embeddings + src_pos_embeddings\n",
    "        encoder_output = self.encoder(encoder_input, src_mask)\n",
    "\n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.tensor, tgt_mask: torch.Tensor) -> torch.Tensor:\n",
    "        tgt_input_embeddings = self.tgt_input_embed(tgt)\n",
    "        tgt_pos_embeddings = self.tgt_pos_embed(tgt)\n",
    "        decoder_input = tgt_input_embeddings + tgt_pos_embeddings\n",
    "        encoder_output = self.encoder(decoder_input, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "    def generate(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.generator(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transformer(config: Config) -> Transformer:\n",
    "    # create the input embedding layer for src and target!\n",
    "    src_input_embeddings = InputEmbeddings(config)\n",
    "    tgt_input_embeddings = InputEmbeddings(config)\n",
    "\n",
    "    # create the positional embedding layer for src and target!\n",
    "    src_pos_embeddings = PositionalEncoding(config)\n",
    "    tgt_pos_embeddings = PositionalEncoding(config)\n",
    "\n",
    "    # create the encoder & decoder blocks!\n",
    "    encoder = Encoder(config)\n",
    "    decoder = Decoder(config)\n",
    "\n",
    "    # create the projection layer\n",
    "    generator = Generator(config)\n",
    "\n",
    "    # initialize a transformer model!\n",
    "    transformer = Transformer(encoder=encoder,\n",
    "                             decoder=decoder,\n",
    "                             generator=generator,\n",
    "                             src_input_embed=src_input_embeddings,\n",
    "                             src_pos_embed=src_pos_embeddings,\n",
    "                             tgt_input_embed=tgt_input_embeddings,\n",
    "                             tgt_pos_embed=tgt_pos_embeddings)\n",
    "    \n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.kaiming_uniform_(p)\n",
    "\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Transformer model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, load_dataset_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datasets_modules.datasets.opus100.256f3196b69901fb0c79810ef468e2c4ed84fbd563719920b1ff1fdc750f7704.opus100.Opus100 at 0x16c52ae10>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset_builder('opus100', name='ar-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7294fbdc40e645228dbbe256fa3d153d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/55.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94274eb31304bdfa538309fe05b3c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3797d445a045919dc9bac9ea5da692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20086e0edcda4e0db3a3991036a902d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_dataset = load_dataset('opus100', name='ar-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1000000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
