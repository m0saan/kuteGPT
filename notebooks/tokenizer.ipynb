{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5170dedc-8365-4e05-a449-3aacb4c1b5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ccaa792-4e42-465f-a033-8b0a7481794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.normalizers import Lowercase, NFKC, Sequence\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers import SentencePieceBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6d2e20b-2ec6-423a-a53f-3ff2030ac095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1000000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset = load_dataset('opus100', name='ar-en')\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90dc115e-1826-45fc-9732-638c3557b4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['translation'],\n",
       "    num_rows: 1000000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a857b1a1-2610-48b5-93fb-8d0d75b07ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'translation': {'ar': '...لقد كان', 'en': 'It was, um...'}},\n",
       " {'ar': 'و هذه؟', 'en': 'And this?'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset['train'][1], raw_dataset['train'][0]['translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be455871-6c9e-4fc3-9ba1-b2067f84223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_texts(dataset, lang=\"en\"):\n",
    "    return (item[\"translation\"][lang] for item in dataset)\n",
    "\n",
    "def build_tokenizer(dataset, lang):\n",
    "    special_tokens = ['[SOS]', '[EOS]', '[PAD]', '[UNK]']\n",
    "    tokenizer = SentencePieceBPETokenizer(unk_token='[UNK]')\n",
    "    tokenizer.train_from_iterator(extract_texts(dataset, lang), special_tokens=special_tokens,\n",
    "                         min_frequency=2,\n",
    "                         show_progress=True,)\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "def get_or_build_tokenizer(config, dataset, lang):\n",
    "    tokenizer_path = Path(config.tokenizer_file.format(lang))\n",
    "    \n",
    "    if tokenizer_path.exists():\n",
    "        return Tokenizer.from_file(str(tokenizer_path))\n",
    "    \n",
    "    tokenizer = build_tokenizer(dataset, lang)\n",
    "    tokenizer.save(str(tokenizer_path))\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "07389bbe-eca8-4365-a198-8e989d396030",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "combined_dataset = concatenate_datasets([raw_dataset['train'], raw_dataset['validation'], raw_dataset['test']])\n",
    "texts = extract_texts(combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d9bc6299-0227-4533-b711-36688fc8cd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src_tokenizer = get_or_build_tokenizer(config, combined_dataset, config.source_lang)\n",
    "target_tokenizer = get_or_build_tokenizer(config, combined_dataset, config.target_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2676776-619a-4a25-9e82-da6337e2c869",
   "metadata": {},
   "source": [
    "### Create the Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3da73bbb-b5e8-48ae-a268-cab78069e666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class EnArDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 dataset: DatasetDict,\n",
    "                 src_tokenizer: Tokenizer,\n",
    "                 target_tokenizer: Tokenizer,\n",
    "                 src_lang: str,\n",
    "                 target_lang: str,\n",
    "                 model_max_length: int) -> None:\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.target_tokenizer = target_tokenizer\n",
    "        self.src_lang = src_lang\n",
    "        self.target_lang = target_lang\n",
    "        self.model_max_length = model_max_length\n",
    "\n",
    "        self.sos_token = torch.tensor([src_tokenizer.token_to_id('[SOS]')], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([src_tokenizer.token_to_id('[EOS]')], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([src_tokenizer.token_to_id('[PAD]')], dtype=torch.int64)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx: any) -> torch.Tensor:\n",
    "        src_target = self.dataset[idx]\n",
    "        src_text = src_target['translation'][self.src_lang]\n",
    "        tgt_text = src_target['translation'][self.target_lang]\n",
    "\n",
    "        encoder_input_tokens = self.src_tokenizer.encode(src_text).ids\n",
    "        decoder_input_tokens = self.target_tokenizer.encode(tgt_text).ids\n",
    "\n",
    "        num_pad_tokens_src = self.model_max_length - len(encoder_input_tokens) - 2\n",
    "        num_pad_tokens_tgt = self.model_max_length - len(decoder_input_tokens) - 1\n",
    "\n",
    "        assert num_pad_tokens_src < 0 and num_pad_tokens_tgt < 0, \"Sentence too long!\"\n",
    "\n",
    "        encoder_inputs = torch.cat([self.sos_token,\n",
    "                                    torch.tensor(encoder_input_tokens, dtype=torch.int64),\n",
    "                                    self.eos_token,\n",
    "                                    torch.tensor([self.pad_token] * num_pad_tokens_src)])\n",
    "\n",
    "        decoder_inputs = torch.cat([self.sos_token,\n",
    "                                    torch.tensor(decoder_input_tokens, dtype=torch.int64),\n",
    "                                    torch.tensor([self.pad_token] * num_pad_tokens_src)])\n",
    "        \n",
    "        label = torch.cat([torch.tensor(decoder_input_tokens, dtype=torch.int64),\n",
    "                           self.eos_token,\n",
    "                           torch.tensor([self.pad_token] * num_pad_tokens_src)])\n",
    "\n",
    "        assert encoder_inputs.size(0) == self.model_max_length\n",
    "        assert decoder_inputs.size(0) == self.model_max_length\n",
    "        assert label.size(0) == self.model_max_length\n",
    "\n",
    "        return {'encoder_inputs': encoder_inputs,\n",
    "                'decoder_inputs': decoder_inputs,\n",
    "                'label': label,\n",
    "                'encoder_mask': ((encoder_inputs != self.pad_token)[None:, ...][None:, ...]).int(),\n",
    "                'decoder_mask': (decoder_inputs != self.pad_token[None:, ...][None:, ...]).int() & causal_mask(sel.model_max_length),\n",
    "                'src_text': src_text,\n",
    "                'tgt_text': tgt_text\n",
    "               }\n",
    "\n",
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ddcedaba-b5cb-467b-8448-3539bfea619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "from typing import Tuple\n",
    "\n",
    "def get_datasets(config, src_tokenizer, target_tokenizer) -> Tuple[EnArDataset, EnArDataset, EnArDataset]:\n",
    "\n",
    "    train_ds = EnArDataset(\n",
    "        raw_dataset['train'],\n",
    "        src_tokenizer,\n",
    "        target_tokenizer,\n",
    "        src_lang=config.source_lang, \n",
    "        target_lang=config.target_lang,\n",
    "        model_max_length=config.model_max_length)\n",
    "\n",
    "    validation_ds = EnArDataset(\n",
    "        raw_dataset['validation'],\n",
    "        src_tokenizer,\n",
    "        target_tokenizer,\n",
    "        src_lang=config.source_lang, \n",
    "        target_lang=config.target_lang,\n",
    "        model_max_length=config.model_max_length)\n",
    "\n",
    "    test_ds = EnArDataset(\n",
    "        raw_dataset['test'],\n",
    "        src_tokenizer,\n",
    "        target_tokenizer,\n",
    "        src_lang=config.source_lang, \n",
    "        target_lang=config.target_lang,\n",
    "        model_max_length=config.model_max_length)\n",
    "    return (train_ds, validation_ds, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6fb9ab34-003c-4693-9786-0cc03796566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders_and_tokenizers(config) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    raw_dataset = load_dataset('opus100', name='ar-en')\n",
    "    combined_dataset = concatenate_datasets([raw_dataset['train'], raw_dataset['validation'], raw_dataset['test']])\n",
    "    texts = extract_texts(combined_dataset)\n",
    "    src_tokenizer = get_or_build_tokenizer(config, combined_dataset, config.source_lang)\n",
    "    target_tokenizer = get_or_build_tokenizer(config, combined_dataset, config.target_lang)\n",
    "    \n",
    "    train_ds, validation_ds, test_ds = get_datasets(config, src_tokenizer, target_tokenizer)\n",
    "    train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "    valid_dl = DataLoader(validation_ds, batch_size=1, shuffle=False)\n",
    "    test_dl = DataLoader(test_ds, batch_size=8, shuffle=False)\n",
    "    return {'train_dl':train_dl,\n",
    "            'valid_dl': valid_dl,\n",
    "            'test_dl': test_dl,\n",
    "            'src_tokenizer': src_tokenizer,\n",
    "            'target_tokenizer': target_tokenizer\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec65bf-665f-4e3f-ac0f-2fac39d3a8ea",
   "metadata": {},
   "source": [
    "### Define The tokenizer config dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c7bf8268-ecf7-499f-a791-58d1cbcda155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "\n",
    "@dataclass\n",
    "class TokenizerConfig:\n",
    "    tokenizer_file: str = 'tokenizer_config_{0}.json'\n",
    "    eos_token: str = '[SOS]'\n",
    "    model_max_length: int = 512\n",
    "    pad_token: str = '[PAD]'\n",
    "    return_tensors: str = 'pt'\n",
    "    separate_vocabs: bool = False\n",
    "    source_lang: str = 'en'\n",
    "    target_lang: str = 'ar'\n",
    "    unk_token: str = '[UNK]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e49ecc57-e028-49bf-aa1f-ce0ecd18cbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenizerConfig(tokenizer_file='tokenizer_config_{0}.json', eos_token='[SOS]', model_max_length=512, pad_token='[PAD]', return_tensors='pt', separate_vocabs=False, source_lang='en', target_lang='ar', unk_token='[UNK]')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = TokenizerConfig()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "991298b2-d929-44ea-81aa-a0d93ff15d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_dl': <torch.utils.data.dataloader.DataLoader at 0x105c60cd0>,\n",
       " 'valid_dl': <torch.utils.data.dataloader.DataLoader at 0x167fc8e50>,\n",
       " 'test_dl': <torch.utils.data.dataloader.DataLoader at 0x168e65790>,\n",
       " 'src_tokenizer': <tokenizers.Tokenizer at 0x1602c2600>,\n",
       " 'target_tokenizer': <tokenizers.Tokenizer at 0x15c9e7400>}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_loaders_and_tokenizers(config, raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e968a7-8d3e-4246-8e6f-c3d5e5de8247",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
